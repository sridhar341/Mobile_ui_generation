{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343fca9-b7a7-4b3d-b548-2b9bc9cfc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import logging\n",
    "import traceback\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Any, DefaultDict, Dict, List, Optional, Tuple\n",
    "import xml.etree.ElementTree as ET\n",
    "from datasets import concatenate_datasets, load_dataset, Dataset, DatasetDict, Features, Value, Sequence, Array3D, ClassLabel\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms.functional as FT\n",
    "import torchvision.transforms as tvT\n",
    "import torchmetrics\n",
    "from einops import rearrange, reduce\n",
    "import gradio as gr\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84712459-41ac-471e-afc0-5cdfcdcbada8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Replace this with the actual link to your Hugging Face dataset\n",
    "dataset_name = \"mrtoy/mobile-ui-design\"  \n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(dataset_name)\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(dataset.keys())  # View the available splits ('train', 'test', etc.)\n",
    "\n",
    "# Example from the 'train' split\n",
    "example = dataset['train'][0]\n",
    "print(example)  # Inspect the contents of a single example \n",
    "print(type(example))  # Check the data type \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce23d4c-c744-422f-81c7-f090d0ea336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW, CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import faiss\n",
    "import openai\n",
    "\n",
    "# Function to explore dataset\n",
    "def explore_dataset():\n",
    "    # Load the dataset from Hugging Face\n",
    "    dataset_dict = load_dataset(\"mrtoy/mobile-ui-design\")\n",
    "\n",
    "    # Access the specific dataset within the DatasetDict object\n",
    "    dataset = dataset_dict[\"train\"]\n",
    "\n",
    "    # Display basic information about the dataset\n",
    "    print(\"Dataset Name:\", \"mrtoy/mobile-ui-design\")\n",
    "    print(\"Number of Instances:\", len(dataset))\n",
    "\n",
    "    # Display the features available in the dataset\n",
    "    print(\"Features:\")\n",
    "    for feature_name, feature_metadata in dataset.features.items():\n",
    "        print(\"\\t\", feature_name, \"-\", feature_metadata)\n",
    "\n",
    "    # Display a sample of the dataset\n",
    "    print(\"Sample Instance:\")\n",
    "    sample_instance = dataset[0]\n",
    "    print(sample_instance)\n",
    "\n",
    "explore_dataset()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('mrtoy/mobile-ui-design', split='train')\n",
    "print(dataset[:5]) \n",
    "\n",
    "# Function to preprocess and load dataset\n",
    "def load_and_preprocess_dataset():\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"mrtoy/mobile-ui-design\")\n",
    "\n",
    "    # Preprocess the dataset if needed\n",
    "    def preprocess(example):\n",
    "        return {\"objects\": example[\"objects\"]} \n",
    "\n",
    "    processed_dataset = dataset.map(preprocess, batched=True)  \n",
    "    return processed_dataset \n",
    "\n",
    "# Define dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        objects = item.get(\"objects\")  \n",
    "        if objects is not None:\n",
    "            if isinstance(objects, list):  \n",
    "                text_list = [obj.get(\"text\", \"\") for obj in objects]  \n",
    "                text_list = [text for text in text_list if text]  \n",
    "                text = \" \".join(text_list)  \n",
    "            elif isinstance(objects, dict):  \n",
    "                text = objects.get(\"text\", \"\") \n",
    "            else:\n",
    "                text = \"\"  \n",
    "        else:\n",
    "            text = \"\"  \n",
    "\n",
    "        # Ensure consistent text format:  \n",
    "        if not isinstance(text, str): \n",
    "            text = \" \".join([str(t) for t in text if t is not None]) \n",
    "\n",
    "        encoded_text = self.tokenizer.encode_plus(text, padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "        return encoded_text\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_and_preprocess_dataset()\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the dataset for training\n",
    "train_dataset = MyDataset(dataset[\"train\"], tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fine-tune BERT on the dataset\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input_ids\"]\n",
    "\n",
    "        # Potential Fix: Reshape inputs if necessary\n",
    "        if inputs.ndim > 2:  \n",
    "           batch_size, seq_length = inputs.shape[:2] \n",
    "           inputs = inputs.reshape(batch_size, seq_length)\n",
    "\n",
    "        # Debugging: Inspect input shapes\n",
    "        print(inputs.shape) \n",
    "\n",
    "model.save_pretrained(\"fine_tuned_bert\") \n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url):\n",
    "    filename = url.split(\"/\")[-1].split(\"#\")[0]\n",
    "    filepath = os.path.join(os.getcwd(), \"files\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"{filename} already exists.\")\n",
    "    else:\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(f\"{filename} downloaded successfully.\")\n",
    "\n",
    "# Function to read JSON file\n",
    "def read_json(filepath):\n",
    "    with open(filepath, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n",
    "\n",
    "# Function to write JSON file\n",
    "def write_json(data, filepath):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "\n",
    "# Function to convert data to COCO format\n",
    "def convert_to_coco(processed_data, save_path):\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    category_set = set()\n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "\n",
    "    for item in processed_data:\n",
    "        image_data = {'id': image_id, 'width': item['width'], 'height': item['height'], 'file_name': item['file_name']}\n",
    "        coco_output['images'].append(image_data)\n",
    "\n",
    "        for obj in item['objects']:\n",
    "            category = obj['category']\n",
    "            category_set.add(category)\n",
    "\n",
    "            coco_output['annotations'].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": list(category_set).index(category) + 1,\n",
    "                \"bbox\": obj['bbox'],\n",
    "                \"area\": obj['bbox'][2] * obj['bbox'][3],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    coco_output['categories'] = [{'id': i, 'name': name} for i, name in enumerate(category_set, start=1)]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(coco_output, f)\n",
    "\n",
    "# Main function to process dataset\n",
    "def main():\n",
    "    \"\"\"Load dataset and process it.\"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = datasets.load_dataset('mrtoy/mobile-ui-design', split='train')\n",
    "\n",
    "    # Initialize lists to store the processed data\n",
    "    images, objects = [], []\n",
    "\n",
    "    # Loop through the dataset items\n",
    "    for item in tqdm(dataset):\n",
    "        # Get image info\n",
    "        width, height = item['width'], item['height']\n",
    "\n",
    "        # Convert category_ids to categories\n",
    "        categories = [dataset.features['category_id'].names[cat_id] for cat_id in item['category_id']]\n",
    "\n",
    "        # Convert the bboxes to a list of dictionaries\n",
    "        objects_list = [{\"category\": cat, \"bbox\": bb} for cat, bb in zip(categories, item['boxes'])]\n",
    "\n",
    "        # Append image and objects info\n",
    "        images.append({'id': item['id'], 'width': width, 'height': height, 'file_name': item['file_name']})\n",
    "        objects.extend(objects_list)\n",
    "\n",
    "    # Combine the images and objects into one list\n",
    "    processed_data = {'images': images, 'objects': objects}\n",
    "\n",
    "    # Convert the processed data to COCO format and save it to a .json file\n",
    "    convert_to_coco(processed_data, save_path='mobile_ui_design.json')\n",
    "\n",
    "# Function to load image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "# Setup for vector storing and retrieval\n",
    "def setup_vector_storage():\n",
    "    openai.api_key = \"API_KEY\"\n",
    "    IMAGE_EMBEDDING_MODEL = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "    dataset = datasets.load_dataset(\"mrtoy/mobile-ui-design\")\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"ui_elements\", \n",
    "        embedding_function=OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "    )\n",
    "    store = InMemoryStore()\n",
    "    retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=\"doc_id\")\n",
    "    llm = OpenAI(openai_api_key=openai.api_key, temperature=0.7) \n",
    "    chain = VectorDBQA.from_llm_and_retriever(llm, retriever)\n",
    "\n",
    "# Function to get CLIP embedding of an image\n",
    "def get_image_embedding(image):\n",
    "    processor = CLIPProcessor.from_pretrained(IMAGE_EMBEDDING_MODEL)\n",
    "    model = CLIPModel.from_pretrained(IMAGE_EMBEDDING_MODEL)\n",
    "    inputs = processor(images=[image], return_tensors=\"pt\")  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.pooler_output.cpu().numpy()[0]  \n",
    "\n",
    "# Function to add a single document (UI element) to the retriever\n",
    "def add_document(image, bounding_boxes, labels, doc_id=None):\n",
    "    doc_id = doc_id or str(uuid.uuid4())\n",
    "    image_embedding = get_image_embedding(image)\n",
    "    document = Document(page_content=image_embedding, metadata={\"doc_id\": doc_id, \"bounding_boxes\": bounding_boxes, \"labels\": labels})\n",
    "    retriever.add_document(document)\n",
    "    return doc_id\n",
    "\n",
    "# Function to find similar UI elements\n",
    "def find_similar_elements(query_image, num_results=3):\n",
    "    query_embedding = get_image_embedding(query_image)\n",
    "    results = retriever.get_relevant_documents(query_embedding, k=num_results)\n",
    "    return results\n",
    "\n",
    "# Function to generate detailed image description using OpenAI API\n",
    "def generate_image_description(user_input_text, temperature=0.7):\n",
    "    prompt = (\n",
    "        f\"User Input: {user_input_text}\\n\\n\"\n",
    "        f\"Detailed Image Description:\"\n",
    "    )\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=250,  \n",
    "        temperature=temperature,  \n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "user_input = \"A login screen with a minimalist aesthetic, featuring a blue color scheme\"\n",
    "detailed_description = generate_image_description(user_input)\n",
    "print(detailed_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896df6ab-401e-4f97-94ad-da85f42c3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import GPT2Config\n",
    "from transformers import ViTFeatureExtractor, ViTForImageGeneration\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Mobile UI Design dataset\n",
    "dataset = load_dataset(\"mrtoy/mobile-ui-design\")\n",
    "\n",
    "# Define device (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and tokenize the language model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=model_config).to(device)\n",
    "\n",
    "# Load and preprocess the computer vision model\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "image_model = ViTForImageGeneration.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device)\n",
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to generate mobile UI design based on input pattern\n",
    "def generate_mobile_ui_design(input_pattern):\n",
    "    # Tokenize input pattern\n",
    "    input_ids = tokenizer.encode(input_pattern, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text based on input pattern\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Generate image based on generated text\n",
    "    inputs = feature_extractor(images=input_pattern, return_tensors=\"pt\").to(device)\n",
    "    image = image_model.generate(**inputs)\n",
    "    \n",
    "    return generated_text, image\n",
    "\n",
    "# Example usage\n",
    "input_pattern = \"A chat messaging app with dark mode\"\n",
    "generated_text, generated_image = generate_mobile_ui_design(input_pattern)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "# Display or save generated_image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
